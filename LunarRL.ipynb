{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Introduccion al problema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %pip install gymnasium"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Windows:\n",
    "\n",
    "method 1 me dio errores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %pip install swig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %pip install gymnasium[box2d]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "method 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: swig in c:\\users\\ivora\\anaconda3\\lib\\site-packages (4.3.1)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    " %pip install swig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: ufal.pybox2d in c:\\users\\ivora\\anaconda3\\lib\\site-packages (2.3.10.4)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    " %pip install ufal.pybox2d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pygame in c:\\users\\ivora\\anaconda3\\lib\\site-packages (2.6.1)Note: you may need to restart the kernel to use updated packages.\n",
      "\n"
     ]
    }
   ],
   "source": [
    " %pip install pygame"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Linux:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %pip install gymnasium[box2d]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://gymnasium.farama.org/environments/box2d/lunar_lander/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from lunar import LunarLanderEnv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tensorflow or Torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting torch\n",
      "  Downloading torch-2.7.0-cp312-cp312-win_amd64.whl.metadata (29 kB)\n",
      "Requirement already satisfied: filelock in c:\\users\\ivora\\anaconda3\\lib\\site-packages (from torch) (3.13.1)\n",
      "Requirement already satisfied: typing-extensions>=4.10.0 in c:\\users\\ivora\\anaconda3\\lib\\site-packages (from torch) (4.11.0)\n",
      "Collecting sympy>=1.13.3 (from torch)\n",
      "  Downloading sympy-1.14.0-py3-none-any.whl.metadata (12 kB)\n",
      "Requirement already satisfied: networkx in c:\\users\\ivora\\anaconda3\\lib\\site-packages (from torch) (3.3)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\ivora\\anaconda3\\lib\\site-packages (from torch) (3.1.4)\n",
      "Requirement already satisfied: fsspec in c:\\users\\ivora\\anaconda3\\lib\\site-packages (from torch) (2024.6.1)\n",
      "Requirement already satisfied: setuptools in c:\\users\\ivora\\anaconda3\\lib\\site-packages (from torch) (75.1.0)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in c:\\users\\ivora\\anaconda3\\lib\\site-packages (from sympy>=1.13.3->torch) (1.3.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\ivora\\anaconda3\\lib\\site-packages (from jinja2->torch) (2.1.3)\n",
      "Downloading torch-2.7.0-cp312-cp312-win_amd64.whl (212.5 MB)\n",
      "   ---------------------------------------- 0.0/212.5 MB ? eta -:--:--\n",
      "    --------------------------------------- 3.7/212.5 MB 21.8 MB/s eta 0:00:10\n",
      "   - -------------------------------------- 8.4/212.5 MB 22.6 MB/s eta 0:00:10\n",
      "   -- ------------------------------------- 12.8/212.5 MB 21.8 MB/s eta 0:00:10\n",
      "   --- ------------------------------------ 17.6/212.5 MB 22.2 MB/s eta 0:00:09\n",
      "   ---- ----------------------------------- 22.5/212.5 MB 22.6 MB/s eta 0:00:09\n",
      "   ----- ---------------------------------- 27.5/212.5 MB 22.7 MB/s eta 0:00:09\n",
      "   ------ --------------------------------- 32.2/212.5 MB 22.5 MB/s eta 0:00:09\n",
      "   ------ --------------------------------- 36.4/212.5 MB 22.3 MB/s eta 0:00:08\n",
      "   ------- -------------------------------- 41.2/212.5 MB 22.4 MB/s eta 0:00:08\n",
      "   -------- ------------------------------- 45.6/212.5 MB 22.3 MB/s eta 0:00:08\n",
      "   --------- ------------------------------ 50.1/212.5 MB 22.3 MB/s eta 0:00:08\n",
      "   ---------- ----------------------------- 55.1/212.5 MB 22.3 MB/s eta 0:00:08\n",
      "   ----------- ---------------------------- 59.5/212.5 MB 22.3 MB/s eta 0:00:07\n",
      "   ------------ --------------------------- 64.0/212.5 MB 22.4 MB/s eta 0:00:07\n",
      "   ------------ --------------------------- 68.7/212.5 MB 22.3 MB/s eta 0:00:07\n",
      "   ------------- -------------------------- 73.1/212.5 MB 22.3 MB/s eta 0:00:07\n",
      "   -------------- ------------------------- 78.1/212.5 MB 22.4 MB/s eta 0:00:06\n",
      "   --------------- ------------------------ 83.1/212.5 MB 22.5 MB/s eta 0:00:06\n",
      "   ---------------- ----------------------- 87.8/212.5 MB 22.5 MB/s eta 0:00:06\n",
      "   ----------------- ---------------------- 91.8/212.5 MB 22.3 MB/s eta 0:00:06\n",
      "   ------------------ --------------------- 96.5/212.5 MB 22.3 MB/s eta 0:00:06\n",
      "   ------------------ -------------------- 101.4/212.5 MB 22.4 MB/s eta 0:00:05\n",
      "   ------------------- ------------------- 106.2/212.5 MB 22.4 MB/s eta 0:00:05\n",
      "   -------------------- ------------------ 111.1/212.5 MB 22.5 MB/s eta 0:00:05\n",
      "   --------------------- ----------------- 115.9/212.5 MB 22.6 MB/s eta 0:00:05\n",
      "   ---------------------- ---------------- 120.8/212.5 MB 22.6 MB/s eta 0:00:05\n",
      "   ----------------------- --------------- 125.8/212.5 MB 22.6 MB/s eta 0:00:04\n",
      "   ----------------------- --------------- 130.0/212.5 MB 22.6 MB/s eta 0:00:04\n",
      "   ------------------------ -------------- 135.0/212.5 MB 22.5 MB/s eta 0:00:04\n",
      "   ------------------------- ------------- 139.5/212.5 MB 22.5 MB/s eta 0:00:04\n",
      "   -------------------------- ------------ 144.4/212.5 MB 22.5 MB/s eta 0:00:04\n",
      "   --------------------------- ----------- 148.6/212.5 MB 22.5 MB/s eta 0:00:03\n",
      "   ---------------------------- ---------- 153.1/212.5 MB 22.4 MB/s eta 0:00:03\n",
      "   ---------------------------- ---------- 157.0/212.5 MB 22.4 MB/s eta 0:00:03\n",
      "   ----------------------------- --------- 161.5/212.5 MB 22.3 MB/s eta 0:00:03\n",
      "   ------------------------------ -------- 165.7/212.5 MB 22.2 MB/s eta 0:00:03\n",
      "   ------------------------------- ------- 170.7/212.5 MB 22.3 MB/s eta 0:00:02\n",
      "   -------------------------------- ------ 175.6/212.5 MB 22.3 MB/s eta 0:00:02\n",
      "   --------------------------------- ----- 180.6/212.5 MB 22.4 MB/s eta 0:00:02\n",
      "   ---------------------------------- ---- 185.9/212.5 MB 22.4 MB/s eta 0:00:02\n",
      "   ----------------------------------- --- 191.1/212.5 MB 22.5 MB/s eta 0:00:01\n",
      "   ----------------------------------- --- 195.8/212.5 MB 22.5 MB/s eta 0:00:01\n",
      "   ------------------------------------ -- 200.5/212.5 MB 22.5 MB/s eta 0:00:01\n",
      "   ------------------------------------- - 205.3/212.5 MB 22.5 MB/s eta 0:00:01\n",
      "   --------------------------------------  210.0/212.5 MB 22.5 MB/s eta 0:00:01\n",
      "   --------------------------------------  212.3/212.5 MB 22.5 MB/s eta 0:00:01\n",
      "   --------------------------------------  212.3/212.5 MB 22.5 MB/s eta 0:00:01\n",
      "   --------------------------------------  212.3/212.5 MB 22.5 MB/s eta 0:00:01\n",
      "   --------------------------------------  212.3/212.5 MB 22.5 MB/s eta 0:00:01\n",
      "   --------------------------------------  212.3/212.5 MB 22.5 MB/s eta 0:00:01\n",
      "   --------------------------------------- 212.5/212.5 MB 20.3 MB/s eta 0:00:00\n",
      "Downloading sympy-1.14.0-py3-none-any.whl (6.3 MB)\n",
      "   ---------------------------------------- 0.0/6.3 MB ? eta -:--:--\n",
      "   ------------------------------- -------- 5.0/6.3 MB 23.2 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 6.3/6.3 MB 21.5 MB/s eta 0:00:00\n",
      "Installing collected packages: sympy, torch\n",
      "  Attempting uninstall: sympy\n",
      "    Found existing installation: sympy 1.13.2\n",
      "    Uninstalling sympy-1.13.2:\n",
      "      Successfully uninstalled sympy-1.13.2\n",
      "Successfully installed sympy-1.14.0 torch-2.7.0\n"
     ]
    }
   ],
   "source": [
    "!pip install torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'gymnasium.spaces.box.Box'>\n",
      "<class 'gymnasium.spaces.discrete.Discrete'>\n"
     ]
    }
   ],
   "source": [
    "# Initialize the environment\n",
    "lunar = LunarLanderEnv()\n",
    "print(type(lunar.env.observation_space))\n",
    "print(type(lunar.env.action_space))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "El espacio de acciones es un valor del 0 al 3 que indica que acciones tomará el modulo lunar para esa iteración.\n",
    "\n",
    "en concreto son las siguientes:\n",
    "\n",
    "|value| action                        |\n",
    "|-----|-------------------------------|\n",
    "| 0   | do nothing                    |\n",
    "| 1   | fire left orientation engine  |\n",
    "| 2   | fire main engine              |\n",
    "| 3   | fire right orientation engine |\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Discrete(4)"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lunar.env.action_space"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "El espacio de observaciones son un conjunto de valores flotantes y booleanos que indica el estado del modulo lunar.\n",
    "\n",
    "en concreto son las siguientes:\n",
    "\n",
    "|value| observation                               |\n",
    "|-----|-------------------------------------------|\n",
    "| 0   | coordenada X (float)                      |\n",
    "| 1   | coordenada Y (float)                      |\n",
    "| 2   | velocidad lineal X (float)                |\n",
    "| 3   | velocidad lineal Y (float)                |\n",
    "| 4   | Angulo en radianes desde -2π a +2π (float)|\n",
    "| 5   | Velocidad angula (float)                  |\n",
    "| 6   | Contacto de la pierna Izquierda (bool)    |\n",
    "| 7   | Contacto de la pierna Derecha (bool)      |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Box([ -2.5        -2.5       -10.        -10.         -6.2831855 -10.\n",
       "  -0.         -0.       ], [ 2.5        2.5       10.        10.         6.2831855 10.\n",
       "  1.         1.       ], (8,), float32)"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# se muestran los valores minimos y maximos del espacio de observaciones.\n",
    "lunar.env.observation_space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "observations: 8, actions: 4\n"
     ]
    }
   ],
   "source": [
    "observation_count = lunar.env.observation_space.shape[0] \n",
    "action_count = lunar.env.action_space.n\n",
    "\n",
    "print(f\"observations: {observation_count}, actions: {action_count}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ -2.5        -2.5       -10.        -10.         -6.2831855 -10.\n",
      "  -0.         -0.       ]\n",
      "[ 2.5        2.5       10.        10.         6.2831855 10.\n",
      "  1.         1.       ]\n"
     ]
    }
   ],
   "source": [
    "#valores minimos y maximos para las observaciones.\n",
    "print(lunar.env.observation_space.low) \n",
    "print(lunar.env.observation_space.high)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sample ofrece una combinacion aleatoria del conjunto."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2\n"
     ]
    }
   ],
   "source": [
    "print(lunar.env.action_space.sample())  # Take a random action"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-2.1996152   1.4910531   1.0308025   8.182502    0.40136677 -2.0072806\n",
      "  0.8625029   0.11971623]\n"
     ]
    }
   ],
   "source": [
    "print(lunar.env.observation_space.sample())  # Sample a random observation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Running a random episode."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_lunar_lander(steps_to_run_before_pause, agent, episodes=1):\n",
    "    \"\"\"\n",
    "    Test the Lunar Lander environment with a given agent.\n",
    "    \n",
    "    Parameters:\n",
    "    steps_to_run_before_pause (int): Number of steps to run before pausing for user input.\n",
    "    agent: The agent to be tested in the environment.\n",
    "    \n",
    "    Returns:\n",
    "    None\n",
    "    \"\"\"\n",
    "    # Initialize the environment\n",
    "    lunar = LunarLanderEnv(render_mode=\"human\")\n",
    "    \n",
    "    if(agent is not None):\n",
    "        # Set the agent's environment\n",
    "        agent.lunar = lunar\n",
    "        \n",
    "    for _ in range(episodes):\n",
    "        counter, score = 0, 0\n",
    "\n",
    "        while True:\n",
    "            if steps_to_run_before_pause != 0 and counter % steps_to_run_before_pause == 0:\n",
    "                input(\"Press Enter to continue...\")\n",
    "\n",
    "            if(agent is not None):\n",
    "                _, reward, done, action = agent.act()\n",
    "                \n",
    "            else:\n",
    "                # Sample a random action from the action space\n",
    "                action = lunar.env.action_space.sample()\n",
    "            \n",
    "                # Take a step in the environment\n",
    "                _, reward, done = lunar.take_action(action, verbose=True)\n",
    "                \n",
    "            score += reward\n",
    "            \n",
    "            counter += 1\n",
    "            \n",
    "            if done:\n",
    "                print(f\"Episode finished, score: {score}\")\n",
    "                break\n",
    "        if(agent is not None):\n",
    "            # Reset the agent's environment for the next episode\n",
    "            agent.lunar.reset()\n",
    "        else:\n",
    "            # Reset the environment for the next episode\n",
    "            lunar.reset()\n",
    "        \n",
    "    # Close the environment\n",
    "    lunar.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step taken: 3, New state: [-0.00474873  1.3849845  -0.23320782 -0.5891735   0.00321516  0.00806732\n",
      "  0.          0.        ], Reward: -0.6544894609418737, Done: False\n",
      "Step taken: 0, New state: [-0.00707684  1.3711271  -0.23320892 -0.61588496  0.00361842  0.00806587\n",
      "  0.          0.        ], Reward: -1.1466379835925977, Done: False\n",
      "Step taken: 2, New state: [-0.00946903  1.3575169  -0.23932724 -0.6048993   0.00372304  0.002093\n",
      "  0.          0.        ], Reward: 1.8526971468570081, Done: False\n",
      "Step taken: 0, New state: [-0.01186113  1.3433065  -0.23932746 -0.6315747   0.00382794  0.00209788\n",
      "  0.          0.        ], Reward: -1.078974700106528, Done: False\n",
      "Step taken: 1, New state: [-0.01434078  1.3285072  -0.2502963  -0.6577604   0.00613124  0.04607079\n",
      "  0.          0.        ], Reward: -1.6203029113122784, Done: False\n",
      "Step taken: 3, New state: [-1.6728496e-02  1.3131138e+00 -2.3875980e-01 -6.8414599e-01\n",
      "  6.1181290e-03 -2.6232505e-04  0.0000000e+00  0.0000000e+00], Reward: -0.5760990226363265, Done: False\n",
      "Step taken: 3, New state: [-0.01902361  1.2971311  -0.22713399 -0.71033597  0.00377471 -0.04687286\n",
      "  0.          0.        ], Reward: -0.3161185273688989, Done: False\n",
      "Step taken: 1, New state: [-0.02139158  1.280553   -0.23627904 -0.73680294  0.00326665 -0.01016231\n",
      "  0.          0.        ], Reward: -1.124812260818685, Done: False\n",
      "Step taken: 1, New state: [-0.02382755  1.2633697  -0.24481979 -0.763707    0.00446989  0.02406716\n",
      "  0.          0.        ], Reward: -1.2592991178811406, Done: False\n",
      "Step taken: 1, New state: [-0.02634115  1.2455828  -0.25454068 -0.7905397   0.00762084  0.06302433\n",
      "  0.          0.        ], Reward: -1.4237975238654588, Done: False\n",
      "Step taken: 1, New state: [-0.0289299   1.2272046  -0.26395324 -0.81684417  0.01265452  0.1006825\n",
      "  0.          0.        ], Reward: -1.4941805635839092, Done: False\n",
      "Step taken: 3, New state: [-0.03145828  1.2082163  -0.2563996  -0.8439618   0.01617228  0.07036174\n",
      "  0.          0.        ], Reward: -0.8515858365116162, Done: False\n",
      "Step taken: 1, New state: [-0.03405323  1.1886249  -0.26473752 -0.8707921   0.02136048  0.10377362\n",
      "  0.          0.        ], Reward: -1.407045476449581, Done: False\n",
      "Step taken: 0, New state: [-0.03664827  1.1684338  -0.26475334 -0.8974627   0.02654687  0.10373728\n",
      "  0.          0.        ], Reward: -1.0636271317212618, Done: False\n",
      "Step taken: 1, New state: [-0.03933048  1.1476349  -0.2756763  -0.9245519   0.0339223   0.14752203\n",
      "  0.          0.        ], Reward: -1.6052579285954198, Done: False\n",
      "Step taken: 2, New state: [-0.04209089  1.1272304  -0.28323534 -0.9070404   0.04103492  0.14226547\n",
      "  0.          0.        ], Reward: 2.4722481694461296, Done: False\n",
      "Step taken: 3, New state: [-0.044771    1.1062204  -0.2731597  -0.9339327   0.04612933  0.1018977\n",
      "  0.          0.        ], Reward: -0.7330885212915905, Done: False\n",
      "Step taken: 2, New state: [-0.04764586  1.0856075  -0.29185197 -0.91626376  0.05045752  0.08657145\n",
      "  0.          0.        ], Reward: 2.458361913883931, Done: False\n",
      "Step taken: 1, New state: [-0.05060234  1.0643879  -0.30209067 -0.9433207   0.05683905  0.12764198\n",
      "  0.          0.        ], Reward: -1.4508374194442115, Done: False\n",
      "Step taken: 0, New state: [-0.05355902  1.042569   -0.30210966 -0.9699896   0.06321841  0.12759885\n",
      "  0.          0.        ], Reward: -1.0169621558928839, Done: False\n",
      "Step taken: 1, New state: [-0.05657835  1.0201414  -0.30996016 -0.99714017  0.0711748   0.15914221\n",
      "  0.          0.        ], Reward: -1.4278653940082495, Done: False\n",
      "Step taken: 1, New state: [-0.05968637  0.99710655 -0.3210724  -1.024285    0.08135802  0.20368254\n",
      "  0.          0.        ], Reward: -1.6888308906684972, Done: False\n",
      "Step taken: 1, New state: [-0.06285457  0.97346723 -0.32860118 -1.0513169   0.09304733  0.23380725\n",
      "  0.          0.        ], Reward: -1.6638894601681204, Done: False\n",
      "Step taken: 3, New state: [-0.06593037  0.94925255 -0.31696036 -1.0768193   0.10236529  0.18637565\n",
      "  0.          0.        ], Reward: -0.6687542799431287, Done: False\n",
      "Step taken: 1, New state: [-0.06908846  0.92440194 -0.3273364  -1.1053042   0.11380821  0.22885838\n",
      "  0.          0.        ], Reward: -1.7440863978148105, Done: False\n",
      "Step taken: 1, New state: [-0.07230606  0.8989417  -0.33477774 -1.1326107   0.12675896  0.25901526\n",
      "  0.          0.        ], Reward: -1.641123726639562, Done: False\n",
      "Step taken: 3, New state: [-0.07545748  0.87289566 -0.3264501  -1.1586076   0.13802305  0.22528179\n",
      "  0.          0.        ], Reward: -0.8538226349715774, Done: False\n",
      "Step taken: 1, New state: [-0.07870217  0.84624094 -0.33812478 -1.185979    0.15163922  0.27232352\n",
      "  0.          0.        ], Reward: -1.7175535062952736, Done: False\n",
      "Step taken: 0, New state: [-0.08194723  0.81898856 -0.33812138 -1.212661    0.16525523  0.2723201\n",
      "  0.          0.        ], Reward: -1.248124263751265, Done: False\n",
      "Step taken: 2, New state: [-0.08544407  0.7922911  -0.36271518 -1.1880525   0.17830753  0.26104635\n",
      "  0.          0.        ], Reward: 2.6870039108712946, Done: False\n",
      "Step taken: 1, New state: [-0.08903494  0.7649812  -0.37445557 -1.2156905   0.19373837  0.30861634\n",
      "  0.          0.        ], Reward: -1.885641094056125, Done: False\n",
      "Step taken: 3, New state: [-0.09254961  0.7370889  -0.36484647 -1.241459    0.20721504  0.26953366\n",
      "  0.          0.        ], Reward: -0.8414897209322862, Done: False\n",
      "Step taken: 0, New state: [-0.09606457  0.70859915 -0.3648419  -1.2681406   0.22069156  0.26953033\n",
      "  0.          0.        ], Reward: -1.1300350940755948, Done: False\n",
      "Step taken: 0, New state: [-0.09957991  0.6795117  -0.36483708 -1.2948222   0.23416792  0.26952696\n",
      "  0.          0.        ], Reward: -1.082508180963515, Done: False\n",
      "Step taken: 0, New state: [-0.10309563  0.64982647 -0.3648319  -1.3215038   0.24764408  0.2695235\n",
      "  0.          0.        ], Reward: -1.0359673205245485, Done: False\n",
      "Step taken: 2, New state: [-0.10670166  0.62105227 -0.37453198 -1.2812654   0.2618572   0.28426224\n",
      "  0.          0.        ], Reward: 4.664415386572938, Done: False\n",
      "Step taken: 2, New state: [-0.11054335  0.5931684  -0.3981758  -1.2418516   0.2762067   0.28698987\n",
      "  0.          0.        ], Reward: 4.018129031778199, Done: False\n",
      "Step taken: 0, New state: [-0.11438541  0.5646872  -0.39816895 -1.268535    0.290556    0.28698573\n",
      "  0.          0.        ], Reward: -1.2556473106025123, Done: False\n",
      "Step taken: 3, New state: [-0.118153    0.53563106 -0.38876718 -1.2938291   0.30295244  0.24792878\n",
      "  0.          0.        ], Reward: -0.6467212394180979, Done: False\n",
      "Step taken: 1, New state: [-0.12200012  0.50593746 -0.39876714 -1.3227116   0.31750974  0.2911463\n",
      "  0.          0.        ], Reward: -1.7327502607639917, Done: False\n",
      "Step taken: 3, New state: [-0.12575808  0.47570217 -0.38734555 -1.3463688   0.3295381   0.24056706\n",
      "  0.          0.        ], Reward: -0.3399857596119762, Done: False\n",
      "Step taken: 1, New state: [-0.12960987  0.44483113 -0.3990851  -1.3752805   0.3440648   0.29053414\n",
      "  0.          0.        ], Reward: -1.7145293672015203, Done: False\n",
      "Step taken: 0, New state: [-0.13346243  0.41336274 -0.39907628 -1.4019638   0.3585913   0.29052982\n",
      "  0.          0.        ], Reward: -1.1214764673460422, Done: False\n",
      "Step taken: 3, New state: [-0.13724089  0.38132924 -0.38969117 -1.426721    0.3711042   0.2502584\n",
      "  0.          0.        ], Reward: -0.5038940457321235, Done: False\n",
      "Step taken: 0, New state: [-0.14101982  0.34869763 -0.38968414 -1.4533998   0.38361698  0.25025573\n",
      "  0.          0.        ], Reward: -0.9123219987439484, Done: False\n",
      "Step taken: 2, New state: [-0.14493828  0.31634417 -0.40396205 -1.4412454   0.39654458  0.25855178\n",
      "  0.          0.        ], Reward: 2.018592959147452, Done: False\n",
      "Step taken: 2, New state: [-0.14940023  0.28468946 -0.4575247  -1.410087    0.4086998   0.24310489\n",
      "  0.          0.        ], Reward: 2.5633957441245174, Done: False\n",
      "Step taken: 2, New state: [-0.15413599  0.25353098 -0.4850123  -1.3881711   0.42103603  0.2467247\n",
      "  0.          0.        ], Reward: 2.145939041029675, Done: False\n",
      "Step taken: 1, New state: [-0.15893897  0.22173208 -0.49342847 -1.4172755   0.43528637  0.2850065\n",
      "  0.          0.        ], Reward: -2.0907048413463074, Done: False\n",
      "Step taken: 2, New state: [-0.16403513  0.19045638 -0.52287453 -1.3942143   0.44976684  0.28960925\n",
      "  0.          0.        ], Reward: 1.5650171966907862, Done: False\n",
      "Step taken: 2, New state: [-0.16941194  0.1595378  -0.55096054 -1.3785006   0.46436062  0.29187548\n",
      "  0.          0.        ], Reward: 0.5567106735073934, Done: False\n",
      "Step taken: 2, New state: [-0.17496291  0.12861373 -0.5683702  -1.3789039   0.4790548   0.2938837\n",
      "  0.          0.        ], Reward: -0.9057176376755536, Done: False\n",
      "Step taken: 3, New state: [-0.18045492  0.09713503 -0.56076956 -1.4031186   0.4919705   0.25831416\n",
      "  0.          0.        ], Reward: -2.0582635311668853, Done: False\n",
      "Step taken: 1, New state: [-0.18602534  0.06500295 -0.57059515 -1.433008    0.5071948   0.30448574\n",
      "  0.          0.        ], Reward: -3.9045080117707287, Done: False\n",
      "Step taken: 3, New state: [-0.19151697  0.03235274 -0.56030387 -1.4553014   0.5198137   0.2523795\n",
      "  0.          0.        ], Reward: -2.7100523268137224, Done: False\n",
      "Step taken: 2, New state: [-1.9724350e-01  8.0202578e-04 -5.6574953e-01 -1.3991776e+00\n",
      "  5.1560676e-01 -7.9126731e-02  0.0000000e+00  1.0000000e+00], Reward: 14.840046793849933, Done: False\n",
      "Step taken: 3, New state: [-0.20381108 -0.0296252  -0.7416807  -0.9199078   0.46135733 -4.43861\n",
      "  0.          1.        ], Reward: -100, Done: True\n",
      "Episode finished, score: -115.50682333726098\n",
      "Environment closed.\n"
     ]
    }
   ],
   "source": [
    "test_lunar_lander(steps_to_run_before_pause=0, agent=None, episodes=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "DQN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (DQN.py, line 100)",
     "output_type": "error",
     "traceback": [
      "Traceback \u001b[1;36m(most recent call last)\u001b[0m:\n",
      "\u001b[0m  File \u001b[0;32m~\\anaconda3\\Lib\\site-packages\\IPython\\core\\interactiveshell.py:3577\u001b[0m in \u001b[0;35mrun_code\u001b[0m\n    exec(code_obj, self.user_global_ns, self.user_ns)\u001b[0m\n",
      "\u001b[1;36m  Cell \u001b[1;32mIn[50], line 1\u001b[1;36m\n\u001b[1;33m    from DQN import DQNAgent\u001b[1;36m\n",
      "\u001b[1;36m  File \u001b[1;32mD:\\Ivo\\Educacion\\Universidad\\3º Curso\\Segundo cuatrimestre\\IA\\Proyecto\\Aprendizaje por refuerzo en modulo lunar\\DQN.py:100\u001b[1;36m\u001b[0m\n\u001b[1;33m    self.target_network.\u001b[0m\n\u001b[1;37m                        ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "from DQN import DQNAgent\n",
    "lunar = LunarLanderEnv(render_mode=None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent = DQNAgent(lunar)\n",
    "agent.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# agent with epsilon = 0.0 (no exploration)\n",
    "agent = DQNAgent(lunar, epsilon=0.0)\n",
    "agent.load_model(\"modelo_DQN.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_lunar_lander(steps_to_run_before_pause=25, agent=agent, episodes=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "REINFORCE <-NO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from REINFORCE import REINFORCEAgent\n",
    "lunar = LunarLanderEnv(render_mode=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# agent = REINFORCEAgent(lunar, episodes=5000)\n",
    "# agent.load_model(\"modelo_REINFORCE.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent = REINFORCEAgent(lunar)\n",
    "agent.load_model(\"modelo_REINFORCE.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_lunar_lander(steps_to_run_before_pause=75, agent=agent, episodes=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
