{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Introduccion al problema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %pip install gymnasium"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Windows:\n",
    "\n",
    "method 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %pip install swig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %pip install gymnasium[box2d]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "method 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %pip install swig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %pip install ufal.pybox2d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %pip install pygame"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Linux:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %pip install gymnasium[box2d]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://gymnasium.farama.org/environments/box2d/lunar_lander/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from lunar import LunarLanderEnv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tensorflow or Torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'gymnasium.spaces.box.Box'>\n",
      "<class 'gymnasium.spaces.discrete.Discrete'>\n"
     ]
    }
   ],
   "source": [
    "# Initialize the environment\n",
    "lunar = LunarLanderEnv()\n",
    "print(type(lunar.env.observation_space))\n",
    "print(type(lunar.env.action_space))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "El espacio de acciones es un valor del 0 al 3 que indica que acciones tomará el modulo lunar para esa iteración.\n",
    "\n",
    "en concreto son las siguientes:\n",
    "\n",
    "|value| action                        |\n",
    "|-----|-------------------------------|\n",
    "| 0   | do nothing                    |\n",
    "| 1   | fire left orientation engine  |\n",
    "| 2   | fire main engine              |\n",
    "| 3   | fire right orientation engine |\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Discrete(4)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lunar.env.action_space"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "El espacio de observaciones son un conjunto de valores flotantes y booleanos que indica el estado del modulo lunar.\n",
    "\n",
    "en concreto son las siguientes:\n",
    "\n",
    "|value| observation                               |\n",
    "|-----|-------------------------------------------|\n",
    "| 0   | coordenada X (float)                      |\n",
    "| 1   | coordenada Y (float)                      |\n",
    "| 2   | velocidad lineal X (float)                |\n",
    "| 3   | velocidad lineal Y (float)                |\n",
    "| 4   | Angulo en radianes desde -2π a +2π (float)|\n",
    "| 5   | Velocidad angula (float)                  |\n",
    "| 6   | Contacto de la pierna Izquierda (bool)    |\n",
    "| 7   | Contacto de la pierna Derecha (bool)      |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Box([ -2.5        -2.5       -10.        -10.         -6.2831855 -10.\n",
       "  -0.         -0.       ], [ 2.5        2.5       10.        10.         6.2831855 10.\n",
       "  1.         1.       ], (8,), float32)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# se muestran los valores minimos y maximos del espacio de observaciones.\n",
    "lunar.env.observation_space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "observations: 8, actions: 4\n"
     ]
    }
   ],
   "source": [
    "observation_count = lunar.env.observation_space.shape[0] \n",
    "action_count = lunar.env.action_space.n\n",
    "\n",
    "print(f\"observations: {observation_count}, actions: {action_count}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ -2.5        -2.5       -10.        -10.         -6.2831855 -10.\n",
      "  -0.         -0.       ]\n",
      "[ 2.5        2.5       10.        10.         6.2831855 10.\n",
      "  1.         1.       ]\n"
     ]
    }
   ],
   "source": [
    "#valores minimos y maximos para las observaciones.\n",
    "print(lunar.env.observation_space.low) \n",
    "print(lunar.env.observation_space.high)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sample ofrece una combinacion aleatoria del conjunto."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3\n"
     ]
    }
   ],
   "source": [
    "print(lunar.env.action_space.sample())  # Take a random action"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-0.3311917  -1.7676712   6.937776   -4.45337    -2.1596286   8.358777\n",
      "  0.9912258   0.27499476]\n"
     ]
    }
   ],
   "source": [
    "print(lunar.env.observation_space.sample())  # Sample a random observation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Running a random episode."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_lunar_lander(steps_to_run_before_pause, agent, episodes=1):\n",
    "    \"\"\"\n",
    "    Test the Lunar Lander environment with a given agent.\n",
    "    \n",
    "    Parameters:\n",
    "    steps_to_run_before_pause (int): Number of steps to run before pausing for user input.\n",
    "    agent: The agent to be tested in the environment.\n",
    "    \n",
    "    Returns:\n",
    "    None\n",
    "    \"\"\"\n",
    "    # Initialize the environment\n",
    "    lunar = LunarLanderEnv(render_mode=None)\n",
    "    score_total = []\n",
    "    if(agent is not None):\n",
    "        # Set the agent's environment\n",
    "        agent.lunar = lunar\n",
    "        \n",
    "    for _ in range(episodes):\n",
    "        counter, score = 0, 0\n",
    "\n",
    "        while True:\n",
    "            if steps_to_run_before_pause != 0 and counter % steps_to_run_before_pause == 0:\n",
    "                input(\"Press Enter to continue...\")\n",
    "\n",
    "            if(agent is not None):\n",
    "                _, reward, done, action = agent.act()\n",
    "                \n",
    "            else:\n",
    "                # Sample a random action from the action space\n",
    "                action = lunar.env.action_space.sample()\n",
    "            \n",
    "                # Take a step in the environment\n",
    "                _, reward, done = lunar.take_action(action, verbose=True)\n",
    "                \n",
    "            score += reward\n",
    "            \n",
    "            counter += 1\n",
    "            \n",
    "            if done:\n",
    "                print(f\"Episode finished, score: {score}\")\n",
    "                score_total.append(score)\n",
    "                break\n",
    "        if(agent is not None):\n",
    "            # Reset the agent's environment for the next episode\n",
    "            agent.lunar.reset()\n",
    "        else:\n",
    "            # Reset the environment for the next episode\n",
    "            lunar.reset()\n",
    "        \n",
    "    # Close the environment\n",
    "    print(f\"Total score over {episodes} episodes: {sum(score_total)/len(score_total)}\")\n",
    "    lunar.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step taken: 0, New state: [-0.01308823  1.4305828  -0.66194016  0.42404655  0.01500874  0.14839913\n",
      "  0.          0.        ], Reward: -0.2097733466805778, Done: False\n",
      "Step taken: 1, New state: [-0.01972141  1.4395199  -0.67306185  0.39707723  0.02465094  0.19286183\n",
      "  0.          0.        ], Reward: -1.4299170163593817, Done: False\n",
      "Step taken: 0, New state: [-0.02635488  1.4478581  -0.6730912   0.37039977  0.03429082  0.19281535\n",
      "  0.          0.        ], Reward: -0.4896929596471864, Done: False\n",
      "Step taken: 2, New state: [-0.03306446  1.4563845  -0.6804208   0.37871203  0.04365255  0.18725197\n",
      "  0.          0.        ], Reward: -3.1461651315636514, Done: False\n",
      "Step taken: 0, New state: [-0.03977442  1.4643121  -0.6804477   0.3520309   0.053013    0.18722644\n",
      "  0.          0.        ], Reward: -0.4855597806811147, Done: False\n",
      "Step taken: 2, New state: [-0.04653168  1.4724548  -0.68507826  0.3615468   0.06228516  0.18546005\n",
      "  0.          0.        ], Reward: -2.912096704843253, Done: False\n",
      "Step taken: 1, New state: [-0.05335731  1.4799966  -0.6936432   0.33469465  0.07326853  0.2196875\n",
      "  0.          0.        ], Reward: -1.4593543009259224, Done: False\n",
      "Step taken: 3, New state: [-0.06012125  1.4869417  -0.68589336  0.308185    0.08268881  0.18842271\n",
      "  0.          0.        ], Reward: 0.1301655256595791, Done: False\n",
      "Step taken: 3, New state: [-0.06682453  1.4933071  -0.678254    0.28245515  0.09054747  0.15718792\n",
      "  0.          0.        ], Reward: 0.24284939358614224, Done: False\n",
      "Step taken: 2, New state: [-0.07347441  1.4998184  -0.67336804  0.28886902  0.0988718   0.16650218\n",
      "  0.          0.        ], Reward: -1.6136688620377015, Done: False\n",
      "Step taken: 0, New state: [-0.08012457  1.5057305  -0.6733912   0.26218832  0.10719413  0.16646172\n",
      "  0.          0.        ], Reward: -0.44850027589518504, Done: False\n",
      "Step taken: 3, New state: [-0.08669195  1.5110601  -0.66297555  0.23641133  0.11339854  0.12409935\n",
      "  0.          0.        ], Reward: 0.6578770095139543, Done: False\n",
      "Step taken: 1, New state: [-0.09334898  1.5157665  -0.67424667  0.20850687  0.12189697  0.16998413\n",
      "  0.          0.        ], Reward: -1.57765078202189, Done: False\n",
      "Step taken: 3, New state: [-0.09991713  1.5198851  -0.66306806  0.18252216  0.12813228  0.12471761\n",
      "  0.          0.        ], Reward: 0.6956775196870535, Done: False\n",
      "Step taken: 2, New state: [-0.10644875  1.5244159  -0.6599923   0.20077686  0.13494417  0.13625024\n",
      "  0.          0.        ], Reward: -1.6899097382549144, Done: False\n",
      "Step taken: 1, New state: [-0.11307554  1.5283284  -0.6719442   0.17302577  0.14417826  0.18469834\n",
      "  0.          0.        ], Reward: -1.7919770775245911, Done: False\n",
      "Step taken: 3, New state: [-0.11961241  1.5316529  -0.66063553  0.14707568  0.15112312  0.13890995\n",
      "  0.          0.        ], Reward: 0.5999220315300999, Done: False\n",
      "Step taken: 2, New state: [-0.12635584  1.5358589  -0.6810201   0.18624115  0.15780862  0.13372204\n",
      "  0.          0.        ], Reward: -4.363490154176037, Done: False\n",
      "Step taken: 1, New state: [-0.13318768  1.5394511  -0.6921003   0.15868397  0.16673404  0.17852427\n",
      "  0.          0.        ], Reward: -1.741113552668877, Done: False\n",
      "Step taken: 0, New state: [-0.14001998  1.5424442  -0.6921261   0.13201186  0.17565703  0.17847607\n",
      "  0.          0.        ], Reward: -0.7052257011334859, Done: False\n",
      "Step taken: 2, New state: [-0.14696646  1.5461287  -0.7037097   0.1626492   0.18475623  0.1820007\n",
      "  0.          0.        ], Reward: -3.4069114092400694, Done: False\n",
      "Step taken: 3, New state: [-0.15383787  1.5492369  -0.69423246  0.13724874  0.19189413  0.14277068\n",
      "  0.          0.        ], Reward: 0.3396276869193866, Done: False\n",
      "Step taken: 2, New state: [-0.16096573  1.5529152  -0.7193918   0.16261655  0.1985539   0.13320743\n",
      "  0.          0.        ], Reward: -4.391187718783146, Done: False\n",
      "Step taken: 2, New state: [-0.1683406   1.5567456  -0.74342334  0.16944432  0.20454928  0.11991777\n",
      "  0.          0.        ], Reward: -3.8527911033515805, Done: False\n",
      "Step taken: 1, New state: [-0.17580137  1.5599276  -0.75486887  0.14017268  0.21342815  0.1775824\n",
      "  0.          0.        ], Reward: -1.8444303448520134, Done: False\n",
      "Step taken: 2, New state: [-0.18343106  1.5639513  -0.77156675  0.17756969  0.22211336  0.17370398\n",
      "  0.          0.        ], Reward: -4.051739583548698, Done: False\n",
      "Step taken: 1, New state: [-0.19114514  1.5673451  -0.7821635   0.14918593  0.23301134  0.21795991\n",
      "  0.          0.        ], Reward: -2.001179924361024, Done: False\n",
      "Step taken: 2, New state: [-0.19891195  1.5712364  -0.7879456   0.17111373  0.24445301  0.22883348\n",
      "  0.          0.        ], Reward: -2.9308571823965393, Done: False\n",
      "Step taken: 3, New state: [-0.20659761  1.5745566  -0.7777055   0.14601208  0.25376156  0.18617111\n",
      "  0.          0.        ], Reward: 0.11339550816333713, Done: False\n",
      "Step taken: 2, New state: [-0.2142353   1.5778832  -0.77355486  0.14612943  0.26375428  0.19985418\n",
      "  0.          0.        ], Reward: -1.3243526353612538, Done: False\n",
      "Step taken: 3, New state: [-0.22180028  1.580651   -0.76430833  0.12158501  0.2717498   0.15991023\n",
      "  0.          0.        ], Reward: 0.12456501519565791, Done: False\n",
      "Step taken: 2, New state: [-0.22955728  1.5838919  -0.78352654  0.14256303  0.27977812  0.16056615\n",
      "  0.          0.        ], Reward: -3.780395991204659, Done: False\n",
      "Step taken: 0, New state: [-0.23731442  1.5865337  -0.7835244   0.11589114  0.2878064   0.16056526\n",
      "  0.          0.        ], Reward: -0.743002959306807, Done: False\n",
      "Step taken: 1, New state: [-0.24515972  1.5885445  -0.7945741   0.08735252  0.2981569   0.20700982\n",
      "  0.          0.        ], Reward: -2.112962664167155, Done: False\n",
      "Step taken: 1, New state: [-0.25309473  1.589911   -0.80586386  0.05815067  0.3109497   0.25585657\n",
      "  0.          0.        ], Reward: -2.4269577297596627, Done: False\n",
      "Step taken: 2, New state: [-0.2614039   1.591756   -0.8426221   0.07944428  0.3231131   0.2432678\n",
      "  0.          0.        ], Reward: -5.671092993900618, Done: False\n",
      "Step taken: 3, New state: [-0.2696258   1.5930454  -0.8315697   0.05517689  0.33288944  0.19552663\n",
      "  0.          0.        ], Reward: 0.02603451220900979, Done: False\n",
      "Step taken: 3, New state: [-0.2777675   1.5937645  -0.82148755  0.03025828  0.3405418   0.15304746\n",
      "  0.          0.        ], Reward: 0.13140871729481887, Done: False\n",
      "Step taken: 3, New state: [-0.2858215   1.5939322  -0.81037647  0.00626261  0.34575957  0.10435607\n",
      "  0.          0.        ], Reward: 0.4559024431818852, Done: False\n",
      "Step taken: 2, New state: [-0.29394835  1.5941954  -0.81804895  0.0103935   0.35138682  0.11254473\n",
      "  0.          0.        ], Reward: -1.8054579938347388, Done: False\n",
      "Step taken: 3, New state: [-0.3020211   1.5938962  -0.8111266  -0.01424349  0.35544828  0.08122925\n",
      "  0.          0.        ], Reward: 0.1312499223776922, Done: False\n",
      "Step taken: 3, New state: [-0.31000596  1.5930439  -0.8000375  -0.03826439  0.35708     0.03263453\n",
      "  0.          0.        ], Reward: 0.7699000343258444, Done: False\n",
      "Step taken: 1, New state: [-0.31807598  1.5915519  -0.8107527  -0.06725599  0.361029    0.07897963\n",
      "  0.          0.        ], Reward: -1.693218356402865, Done: False\n",
      "Step taken: 1, New state: [-0.32620925  1.589422   -0.8187496  -0.096035    0.3667581   0.11458229\n",
      "  0.          0.        ], Reward: -1.6381236793019684, Done: False\n",
      "Step taken: 0, New state: [-0.33434263  1.5866926  -0.8187482  -0.12270425  0.3724872   0.11458206\n",
      "  0.          0.        ], Reward: -0.8242224933459283, Done: False\n",
      "Step taken: 3, New state: [-0.34241915  1.5833933  -0.8115659  -0.14766055  0.37664148  0.08308567\n",
      "  0.          0.        ], Reward: 0.008825542179267815, Done: False\n",
      "Step taken: 3, New state: [-0.35043773  1.5795251  -0.80425817 -0.17255484  0.3791887   0.05094432\n",
      "  0.          0.        ], Reward: 0.1544323325340156, Done: False\n",
      "Step taken: 1, New state: [-0.35853562  1.5750003  -0.81436235 -0.20232473  0.38404986  0.0972236\n",
      "  0.          0.        ], Reward: -1.9080665304769286, Done: False\n",
      "Step taken: 0, New state: [-0.3666336   1.5698757  -0.8143612  -0.22899327  0.38891104  0.09722328\n",
      "  0.          0.        ], Reward: -0.8512363456565595, Done: False\n",
      "Step taken: 2, New state: [-0.37495652  1.5652157  -0.83710563 -0.20843783  0.39405447  0.10286798\n",
      "  0.          0.        ], Reward: -2.224569872290476, Done: False\n",
      "Step taken: 1, New state: [-0.38335943  1.5598956  -0.84730226 -0.23841077  0.40156314  0.15017323\n",
      "  0.          0.        ], Reward: -2.2159342069922148, Done: False\n",
      "Step taken: 3, New state: [-0.39168715  1.5540117  -0.8378569  -0.2629591   0.40701815  0.1091005\n",
      "  0.          0.        ], Reward: -0.0004061163817527802, Done: False\n",
      "Step taken: 1, New state: [-0.40010113  1.5474815  -0.8486905  -0.29234657  0.41487202  0.15707728\n",
      "  0.          0.        ], Reward: -2.3386946972184703, Done: False\n",
      "Step taken: 3, New state: [-0.40845925  1.5403978  -0.8415241  -0.31652737  0.42103353  0.12323014\n",
      "  0.          0.        ], Reward: -0.3178836066070676, Done: False\n",
      "Step taken: 1, New state: [-0.41687888  1.5326642  -0.8493673  -0.34594738  0.4290538   0.16040592\n",
      "  0.          0.        ], Reward: -2.1068510943274235, Done: False\n",
      "Step taken: 2, New state: [-0.42561826  1.5255381  -0.8815001  -0.31905034  0.43727836  0.16449113\n",
      "  0.          0.        ], Reward: -2.702039536304471, Done: False\n",
      "Step taken: 3, New state: [-0.4342946   1.5178593  -0.8734544  -0.34309956  0.44362426  0.12691826\n",
      "  0.          0.        ], Reward: -0.2577468135105778, Done: False\n",
      "Step taken: 2, New state: [-0.4430627   1.5103699  -0.88321435 -0.3349143   0.45063102  0.14013605\n",
      "  0.          0.        ], Reward: -1.1411083355741198, Done: False\n",
      "Step taken: 0, New state: [-0.45183095  1.5022811  -0.8832118  -0.3615847   0.45763776  0.14013556\n",
      "  0.          0.        ], Reward: -1.1529379259598045, Done: False\n",
      "Step taken: 0, New state: [-0.46059942  1.4935927  -0.88320905 -0.3882551   0.4646445   0.14013511\n",
      "  0.          0.        ], Reward: -1.1668040269047424, Done: False\n",
      "Step taken: 1, New state: [-0.4694417   1.4842522  -0.8925228  -0.4179398   0.47382933  0.1836969\n",
      "  0.          0.        ], Reward: -2.3955896491665087, Done: False\n",
      "Step taken: 2, New state: [-0.47861773  1.4756418  -0.9263109  -0.38569388  0.48351195  0.19365288\n",
      "  0.          0.        ], Reward: -2.5151064075037537, Done: False\n",
      "Step taken: 2, New state: [-0.4880908   1.4668435  -0.955417   -0.39390603  0.4925691   0.18114275\n",
      "  0.          0.        ], Reward: -3.6687942589108387, Done: False\n",
      "Step taken: 2, New state: [-0.49776116  1.4582368  -0.9754073  -0.38555253  0.5019769   0.18815671\n",
      "  0.          0.        ], Reward: -2.2749909928119676, Done: False\n",
      "Step taken: 2, New state: [-0.5075473   1.4498062  -0.9875534  -0.3779921   0.51206654  0.20179276\n",
      "  0.          0.        ], Reward: -1.689814968637836, Done: False\n",
      "Step taken: 3, New state: [-0.5172781   1.4408367  -0.9803197  -0.4013892   0.5202875   0.16441785\n",
      "  0.          0.        ], Reward: -0.5208391276046018, Done: False\n",
      "Step taken: 1, New state: [-0.52706355  1.4312295  -0.98716956 -0.4303101   0.5301214   0.19667812\n",
      "  0.          0.        ], Reward: -2.2017985923972945, Done: False\n",
      "Step taken: 3, New state: [-0.53676724  1.4210935  -0.97671187 -0.45298883  0.5373865   0.14530352\n",
      "  0.          0.        ], Reward: -0.12243963418282419, Done: False\n",
      "Step taken: 3, New state: [-0.5464129   1.4104044  -0.9693197  -0.47696123  0.542859    0.10945044\n",
      "  0.          0.        ], Reward: -0.290177541280882, Done: False\n",
      "Step taken: 0, New state: [-0.5560587   1.3991159  -0.96931773 -0.50363004  0.5483315   0.1094502\n",
      "  0.          0.        ], Reward: -1.0522266060707466, Done: False\n",
      "Step taken: 2, New state: [-0.56601465  1.3879358  -1.0002463  -0.49880105  0.5537185   0.10774057\n",
      "  0.          0.        ], Reward: -2.710677941619326, Done: False\n",
      "Step taken: 2, New state: [-0.5764525   1.3768893  -1.0478114  -0.49261674  0.55840045  0.09363931\n",
      "  0.          0.        ], Reward: -4.157445884669857, Done: False\n",
      "Step taken: 3, New state: [-0.58683413  1.3652967  -1.0406158  -0.5162513   0.5612534   0.05705941\n",
      "  0.          0.        ], Reward: -0.033660163144274974, Done: False\n",
      "Step taken: 0, New state: [-0.5972159   1.3531042  -1.0406153  -0.5429186   0.5641064   0.05705893\n",
      "  0.          0.        ], Reward: -0.7914680344077851, Done: False\n",
      "Step taken: 3, New state: [-0.60753316  1.3403847  -1.0322573  -0.5655365   0.5647269   0.01240928\n",
      "  0.          0.        ], Reward: 0.31827744603268227, Done: False\n",
      "Step taken: 0, New state: [-0.6178504   1.3270652  -1.0322573  -0.59220314  0.5653474   0.01240927\n",
      "  0.          0.        ], Reward: -0.5866510715528079, Done: False\n",
      "Step taken: 1, New state: [-0.62824285  1.3130864  -1.0417428  -0.6223375   0.568273    0.05851327\n",
      "  0.          0.        ], Reward: -1.8432238624984894, Done: False\n",
      "Step taken: 0, New state: [-0.6386353   1.2985077  -1.0417422  -0.6490048   0.5711987   0.05851318\n",
      "  0.          0.        ], Reward: -0.8234348477590743, Done: False\n",
      "Step taken: 2, New state: [-0.6493838   1.2842344  -1.0774448  -0.63549054  0.5742484   0.06099305\n",
      "  0.          0.        ], Reward: -2.1598717838378887, Done: False\n",
      "Step taken: 2, New state: [-0.6605143   1.270466   -1.1159542  -0.6131807   0.5776657   0.06834661\n",
      "  0.          0.        ], Reward: -2.1670353966781475, Done: False\n",
      "Step taken: 0, New state: [-0.67164487  1.2560979  -1.1159534  -0.6398481   0.581083    0.06834646\n",
      "  0.          0.        ], Reward: -0.89528213023641, Done: False\n",
      "Step taken: 0, New state: [-0.68277544  1.2411299  -1.1159526  -0.6665155   0.5845003   0.06834611\n",
      "  0.          0.        ], Reward: -0.9036342260575907, Done: False\n",
      "Step taken: 3, New state: [-0.6938509   1.2256287  -1.1087599  -0.6894812   0.58594453  0.02888495\n",
      "  0.          0.        ], Reward: 0.05836526395754163, Done: False\n",
      "Step taken: 3, New state: [-0.704851    1.2096049  -1.0990977  -0.7117675   0.58486164 -0.02165827\n",
      "  0.          0.        ], Reward: 0.5414939407946531, Done: False\n",
      "Step taken: 0, New state: [-0.7158511   1.192981   -1.0990976  -0.7384342   0.5837787  -0.02165874\n",
      "  0.          0.        ], Reward: -0.4891161974863394, Done: False\n",
      "Step taken: 2, New state: [-0.72702444  1.1764603  -1.116864   -0.7340497   0.58322203 -0.01113272\n",
      "  0.          0.        ], Reward: -0.6514921757000594, Done: False\n",
      "Step taken: 0, New state: [-0.7381978   1.1593394  -1.1168638  -0.7607163   0.5826654  -0.01113287\n",
      "  0.          0.        ], Reward: -0.5707483116934782, Done: False\n",
      "Step taken: 1, New state: [-0.7494378   1.1415565  -1.1253563  -0.7909504   0.5842701   0.03209468\n",
      "  0.          0.        ], Reward: -1.7261611905368273, Done: False\n",
      "Step taken: 0, New state: [-0.7606778   1.1231737  -1.1253561  -0.81761724  0.58587486  0.03209471\n",
      "  0.          0.        ], Reward: -0.8051159925803404, Done: False\n",
      "Step taken: 2, New state: [-0.7721865   1.1052289  -1.1528423  -0.7984184   0.5882141   0.04678432\n",
      "  0.          0.        ], Reward: -0.8387332754521595, Done: False\n",
      "Step taken: 3, New state: [-7.8362620e-01  1.0867538e+00 -1.1439953e+00 -8.2112670e-01\n",
      "  5.8824974e-01  7.1306922e-04  0.0000000e+00  0.0000000e+00], Reward: 0.2253179206963705, Done: False\n",
      "Step taken: 0, New state: [-7.9506582e-01  1.0676788e+00 -1.1439953e+00 -8.4779334e-01\n",
      "  5.8828539e-01  7.1287603e-04  0.0000000e+00  0.0000000e+00], Reward: -0.7125561441546893, Done: False\n",
      "Step taken: 0, New state: [-8.0650550e-01  1.0480037e+00 -1.1439953e+00 -8.7445998e-01\n",
      "  5.8832103e-01  7.1280129e-04  0.0000000e+00  0.0000000e+00], Reward: -0.7290105950775114, Done: False\n",
      "Step taken: 1, New state: [-0.8180109   1.0276651  -1.1523938  -0.9047605   0.5905174   0.04392751\n",
      "  0.          0.        ], Reward: -1.8767924564836835, Done: False\n",
      "Step taken: 1, New state: [-0.8295825   1.0066651  -1.1608081  -0.934975    0.5948594   0.08684015\n",
      "  0.          0.        ], Reward: -2.0998250410569583, Done: False\n",
      "Step taken: 1, New state: [-0.84123176  0.98498887 -1.170707   -0.96601397  0.60176855  0.13818227\n",
      "  0.          0.        ], Reward: -2.5375487307251476, Done: False\n",
      "Step taken: 3, New state: [-0.852823    0.96276885 -1.1632956  -0.9894766   0.60676914  0.10001209\n",
      "  0.          0.        ], Reward: -0.5528869890717647, Done: False\n",
      "Step taken: 1, New state: [-0.86448014  0.9398779  -1.1717442  -1.0201821   0.61402947  0.14520708\n",
      "  0.          0.        ], Reward: -2.481149642632913, Done: False\n",
      "Step taken: 1, New state: [-0.87621075  0.91632545 -1.1809542  -1.0505246   0.6235965   0.19134061\n",
      "  0.          0.        ], Reward: -2.767173907723104, Done: False\n",
      "Step taken: 3, New state: [-0.8878925   0.89223677 -1.1745303  -1.0736762   0.6313432   0.15493383\n",
      "  0.          0.        ], Reward: -0.9693985682175981, Done: False\n",
      "Step taken: 3, New state: [-0.8995051   0.86762637 -1.1656021  -1.09592     0.6366604   0.10634451\n",
      "  0.          0.        ], Reward: -0.5201623166190348, Done: False\n",
      "Step taken: 2, New state: [-0.911395    0.84292233 -1.1932311  -1.1000634   0.6418745   0.10428174\n",
      "  0.          0.        ], Reward: -2.2939648987357257, Done: False\n",
      "Step taken: 2, New state: [-0.9235219   0.8185354  -1.2175678  -1.0863101   0.64790386  0.12058741\n",
      "  0.          0.        ], Reward: -1.0438322075905717, Done: False\n",
      "Step taken: 2, New state: [-0.936124    0.79436904 -1.2648962  -1.0764338   0.65370774  0.11607826\n",
      "  0.          0.        ], Reward: -3.1686003395969236, Done: False\n",
      "Step taken: 3, New state: [-0.9486802   0.76966983 -1.2588158  -1.0993856   0.65769124  0.07966987\n",
      "  0.          0.        ], Reward: -0.8555444351185588, Done: False\n",
      "Step taken: 1, New state: [-0.9612952   0.74429125 -1.2664816  -1.1305171   0.6639209   0.1245911\n",
      "  0.          0.        ], Reward: -2.700325727566819, Done: False\n",
      "Step taken: 3, New state: [-0.9738658   0.71837026 -1.2607017  -1.153953    0.66848665  0.09131494\n",
      "  0.          0.        ], Reward: -1.0692510650376608, Done: False\n",
      "Step taken: 2, New state: [-0.9871278   0.693008   -1.329587   -1.1290034   0.67275846  0.08543591\n",
      "  0.          0.        ], Reward: -3.839622429798726, Done: False\n",
      "Step taken: 1, New state: [-1.0004611   0.6669545  -1.338819   -1.1608711   0.6797004   0.13883851\n",
      "  0.          0.        ], Reward: -100, Done: True\n",
      "Episode finished, score: -250.32514724958364\n",
      "Total score over 1 episodes: -250.32514724958364\n",
      "Environment closed.\n"
     ]
    }
   ],
   "source": [
    "test_lunar_lander(steps_to_run_before_pause=0, agent=None, episodes=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "DQN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from DQN import DQNAgent\n",
    "lunar = LunarLanderEnv(render_mode=None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cpu\n"
     ]
    }
   ],
   "source": [
    "agent = DQNAgent(lunar)\n",
    "agent.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cpu\n",
      "QNetwork:\n",
      " DQN(\n",
      "  (model): Sequential(\n",
      "    (0): Linear(in_features=8, out_features=256, bias=True)\n",
      "    (1): ReLU()\n",
      "    (2): Linear(in_features=256, out_features=256, bias=True)\n",
      "    (3): ReLU()\n",
      "    (4): Linear(in_features=256, out_features=4, bias=True)\n",
      "  )\n",
      ")\n",
      "Model loaded from modelo_DQN.h5\n"
     ]
    }
   ],
   "source": [
    "# agent with epsilon = 0.0 (no exploration)\n",
    "agent = DQNAgent(lunar, epsilon=0.0)\n",
    "agent.load_model(\"modelo_DQN.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode finished, score: -26.94699531287399\n",
      "Episode finished, score: -24.207171171179876\n",
      "Episode finished, score: -32.931736018297656\n",
      "Episode finished, score: -9.815367005881512\n",
      "Episode finished, score: 3.6127161852417675\n",
      "Episode finished, score: 7.233316595495387\n",
      "Episode finished, score: -1.7151160877529266\n",
      "Episode finished, score: -1.1398311757982882\n",
      "Episode finished, score: -34.259377430174546\n",
      "Episode finished, score: -14.539112834668106\n",
      "Episode finished, score: -2.4748684326585857\n",
      "Episode finished, score: -15.152124734064628\n",
      "Episode finished, score: -41.58915296138186\n",
      "Episode finished, score: -19.068839891689525\n",
      "Episode finished, score: -37.56357135395363\n",
      "Episode finished, score: -19.003499039106316\n",
      "Episode finished, score: -16.91249309281332\n",
      "Episode finished, score: -0.5925008532810949\n",
      "Episode finished, score: -25.24383622027657\n",
      "Episode finished, score: 19.956399577751736\n",
      "Episode finished, score: -30.433635860197487\n",
      "Episode finished, score: -6.813149603464224\n",
      "Episode finished, score: -30.557378235633315\n",
      "Episode finished, score: 1.5008876658573678\n",
      "Episode finished, score: -25.761562067312507\n",
      "Episode finished, score: -8.398579993967243\n",
      "Episode finished, score: -29.861128889051834\n",
      "Episode finished, score: -26.95718212231737\n",
      "Episode finished, score: -14.539880633532952\n",
      "Episode finished, score: -43.3505792497082\n",
      "Episode finished, score: 5.453667286576613\n",
      "Episode finished, score: 9.0844105826589\n",
      "Episode finished, score: -15.283121283544904\n",
      "Episode finished, score: 4.455895269830117\n",
      "Episode finished, score: 6.601984559616481\n",
      "Episode finished, score: -34.38895394211957\n",
      "Episode finished, score: -4.869176213472187\n",
      "Episode finished, score: -47.73081234315933\n",
      "Episode finished, score: 44.029596796592465\n",
      "Episode finished, score: -23.046228903131162\n",
      "Episode finished, score: 10.76151932072068\n",
      "Episode finished, score: 12.726044989390203\n",
      "Episode finished, score: -20.871957427808\n",
      "Episode finished, score: -48.1435075162473\n",
      "Episode finished, score: -11.674173479534046\n",
      "Episode finished, score: -3.7808642768128484\n",
      "Episode finished, score: -16.738368041119674\n",
      "Episode finished, score: -33.75851531564842\n",
      "Episode finished, score: 20.449239587429147\n",
      "Episode finished, score: -1.2262994294850482\n",
      "Episode finished, score: -44.80127475454452\n",
      "Episode finished, score: -8.733352042420368\n",
      "Episode finished, score: -2.647292118293209\n",
      "Episode finished, score: -3.787629950301114\n",
      "Episode finished, score: 28.80622411575954\n",
      "Episode finished, score: -8.707770940281799\n",
      "Episode finished, score: 1.3882822825405108\n",
      "Episode finished, score: -37.56991951408962\n",
      "Episode finished, score: -28.796174829353202\n",
      "Episode finished, score: -11.83462229423674\n",
      "Episode finished, score: 25.433003767375173\n",
      "Episode finished, score: -4.751868722046988\n",
      "Episode finished, score: -8.652090146885268\n",
      "Episode finished, score: -38.57239061314536\n",
      "Episode finished, score: -18.086922172376585\n",
      "Episode finished, score: -8.664195236205925\n",
      "Episode finished, score: -16.599901650835328\n",
      "Episode finished, score: -58.36774202592521\n",
      "Episode finished, score: 15.003355192774459\n",
      "Episode finished, score: -11.783419629937907\n",
      "Episode finished, score: -18.462522917972922\n",
      "Episode finished, score: 0.7327356763905801\n",
      "Episode finished, score: -16.913933338286707\n",
      "Episode finished, score: -17.84851896285127\n",
      "Episode finished, score: -60.163409878526195\n",
      "Episode finished, score: -6.595021289842322\n",
      "Episode finished, score: 10.728281178423753\n",
      "Episode finished, score: -4.568424353519799\n",
      "Episode finished, score: -14.421777409131787\n",
      "Episode finished, score: 13.150322979417592\n",
      "Episode finished, score: -16.873913721763905\n",
      "Episode finished, score: -18.42605059310837\n",
      "Episode finished, score: 2.645292132669286\n",
      "Episode finished, score: -10.656286157484729\n",
      "Episode finished, score: -19.119857669101496\n",
      "Episode finished, score: 11.488107519898115\n",
      "Episode finished, score: 2.778527510814124\n",
      "Episode finished, score: -46.482929317329265\n",
      "Episode finished, score: -23.483306825577763\n",
      "Episode finished, score: -1.587805708362683\n",
      "Episode finished, score: 3.684204655772519\n",
      "Episode finished, score: 8.96980320316159\n",
      "Episode finished, score: -40.373206682325765\n",
      "Episode finished, score: -27.579876237195606\n",
      "Episode finished, score: 19.789521765523116\n",
      "Episode finished, score: -6.7292600306748245\n",
      "Episode finished, score: 4.355042978780826\n",
      "Episode finished, score: -27.017186827943004\n",
      "Episode finished, score: -27.604660636276176\n",
      "Episode finished, score: -33.6559444486121\n",
      "Total score over 100 episodes: -12.574427247134238\n",
      "Environment closed.\n"
     ]
    }
   ],
   "source": [
    "test_lunar_lander(steps_to_run_before_pause=0, agent=agent, episodes=100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "REINFORCE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'REINFORCE'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[172], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mREINFORCE\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m REINFORCEAgent\n\u001b[0;32m      2\u001b[0m lunar \u001b[38;5;241m=\u001b[39m LunarLanderEnv(render_mode\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m)\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'REINFORCE'"
     ]
    }
   ],
   "source": [
    "from REINFORCE import REINFORCEAgent\n",
    "lunar = LunarLanderEnv(render_mode=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# agent = REINFORCEAgent(lunar, episodes=5000)\n",
    "# agent.load_model(\"modelo_REINFORCE.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "e:\\Discovirtual-us\\IA personal\\Aprendizaje por refuerzo trabajo 2024-2025\\VersionAlumnos\\REINFORCE.py:99: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  self.actor_net.load_state_dict(torch.load(path))\n"
     ]
    }
   ],
   "source": [
    "agent = REINFORCEAgent(lunar)\n",
    "agent.load_model(\"modelo_REINFORCE.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode finished, score: 264.42781863081314\n",
      "Environment closed.\n"
     ]
    }
   ],
   "source": [
    "test_lunar_lander(steps_to_run_before_pause=75, agent=agent, episodes=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
